{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "for _ in range(50):\n",
    "    start = random.randint(0, 97)  # Random start to generate diverse sequences\n",
    "    seq = [start, start+1, start+2]\n",
    "    train_data.append(seq)\n",
    "    train_label.append(start+3)  # Next number in sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "[96, 97, 98]\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(train_label))\n",
    "print(train_data[1])\n",
    "print(train_label[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X = torch.FloatTensor(train_data).unsqueeze(-1)  # Shape: [50, 3, 1]\n",
    "y = torch.FloatTensor(train_label)\n",
    "\n",
    "# Simple RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=1, hidden_size=16, batch_first=True)\n",
    "        self.linear = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        return self.linear(output[:, -1, :]).squeeze()\n",
    "\n",
    "# Create and train model\n",
    "model = SimpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 3660.5479\n",
      "Epoch: 50 | Loss: 2864.2576\n",
      "Epoch: 100 | Loss: 2230.7095\n",
      "Epoch: 150 | Loss: 1732.7688\n",
      "Epoch: 200 | Loss: 1344.2004\n",
      "Epoch: 250 | Loss: 1040.6659\n",
      "Epoch: 300 | Loss: 805.0283\n",
      "Epoch: 350 | Loss: 621.6252\n",
      "Epoch: 400 | Loss: 480.4293\n",
      "Epoch: 450 | Loss: 373.4489\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    prediction = model(X)\n",
    "    loss = loss_fn(prediction, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch} | Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence [7, 8, 9] -> Predicted: 10.0128, Expected: 10\n",
      "\n",
      "Sequence [11, 12, 13] -> Predicted: 13.9411, Expected: 14\n",
      "\n",
      "Sequence [20, 21, 22] -> Predicted: 22.9613, Expected: 23\n"
     ]
    }
   ],
   "source": [
    "# Test several sequences\n",
    "test_sequences = [\n",
    "    [7,8,9],\n",
    "    [11,12,13],\n",
    "    [20,21,22]\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for seq in test_sequences:\n",
    "        test_input = torch.FloatTensor(seq).reshape(1, 3, 1)\n",
    "        pred = model(test_input)\n",
    "        print(f'\\nSequence {seq} -> Predicted: {pred.item():.4f}, Expected: {seq[-1]+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://www.tensorflow.org/text/guide/word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import kagglehub\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: /home/azureuser/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probably want to change the name of the directory w/ \"mv IMDB Dataset.csv dataset.csv\" \n",
    "path = '/home/azureuser/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv'\n",
    "os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what mone...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                    review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me...   \n",
       "1  A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire p...   \n",
       "2  I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue i...   \n",
       "3  Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenl...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what mone...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(path)\n",
    "print(len(dataset))\n",
    "print(dataset.shape)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'john', 'doe']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "\n",
    "    text = text.replace('<br />', ' ')\n",
    "    text = text.replace('<br/>', ' ')\n",
    "    text = text.replace('<br>', ' ')\n",
    "    text = text.lower()    \n",
    "    for punctuation in \".,!?:;:\":\n",
    "        text = text.replace(punctuation, f'') # Add space before and after the punctuation\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "text = \"My name is John Doe!\"\n",
    "print(tokenizer(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', \"you'll\", 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'manyaryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'moreso', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', \"wouldn't\", 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romanceoz', \"doesn't\", 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', \"couldn't\", 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', '(crooked', 'guards', \"who'll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', \"who'll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience)', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewingthats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']\n"
     ]
    }
   ],
   "source": [
    "sample_review = dataset['review'][0]\n",
    "print(tokenizer(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(texts, max_words=10000):\n",
    "    word_counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = tokenizer(text)\n",
    "        word_counter.update(tokens)\n",
    "\n",
    "    vocab = ['<PAD>', '<UNK>']\n",
    "    vocab += [word for word, count in word_counter.most_common(max_words)]\n",
    "\n",
    "    word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(dataset['review'])\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "vocab_20 = list(vocab.items())[:10]\n",
    "for word, idx in vocab_20:\n",
    "    print(f'{word}: {idx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create custom dataset \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class MovieReview(Dataset):\n",
    "    def __init__(self, reviews, labels, vocab, max_len=200):\n",
    "        self.reviews = list(reviews)\n",
    "        self.labels = list(labels) \n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = tokenizer(review)[:self.max_len]\n",
    "\n",
    "        indices = [self.vocab.get(token, 1) for token in tokens] \n",
    "\n",
    "        if len(indices) < self.max_len:\n",
    "            indices += [0] * (self.max_len - len(indices))\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(indices),\n",
    "            'labels': torch.tensor(1 if label == 'positive' else 0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset['review'], dataset['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 37500 x_test: 12500 y_train: 37500 y_test: 12500\n"
     ]
    }
   ],
   "source": [
    "print(f'x_train: {len(x_train)} x_test: {len(x_test)} y_train: {len(y_train)} y_test: {len(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MovieReview(x_train, y_train, vocab)\n",
    "test_dataset = MovieReview(x_test, y_test, vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   9,    5,   51,  ...,    0,    0,    0],\n",
      "        [  49,   20,   74,  ...,    0,    0,    0],\n",
      "        [   8,   11,   50,  ...,   46,  146, 2950],\n",
      "        ...,\n",
      "        [   1,  228,   36,  ...,   42,    6,  924],\n",
      "        [   9,  382, 6066,  ...,  763,   13, 5409],\n",
      "        [   9,    5,   26,  ...,   63,    4, 2579]])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "sample_batch = next(iter(train_loader))\n",
    "print(sample_batch['input_ids'])\n",
    "print(sample_batch['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embeddings): Embedding(10000, 200)\n",
       "  (lstm): LSTM(200, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=200, hidden_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                            hidden_size=hidden_dim, \n",
    "                            num_layers=n_layers,\n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input shape:\", x.shape)\n",
    "\n",
    "        embedded = self.dropout(self.embeddings(x))\n",
    "        print(\"Embedded shape:\", embedded.shape)\n",
    "\n",
    "\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        print(f'lstm output shape: {lstm_out.shape}')\n",
    "\n",
    "        out = self.linear1(lstm_out[:, -1, :])\n",
    "        print(\"Output shape:\", out.shape)\n",
    "\n",
    "\n",
    "        torch.sigmoid(out)\n",
    "\n",
    "model = SentimentLSTM(vocab_size=len(vocab))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Get data\n",
    "            inputs = batch['input_ids']\n",
    "            labels = batch['labels'].float()\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            print(f'...{outputs.shape}')\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 200])\n",
      "Embedded shape: torch.Size([32, 200, 200])\n",
      "lstm output shape: torch.Size([32, 200, 512])\n",
      "Output shape: torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[112], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[111], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(out)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Create model and optimizer\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m SentimentRNN(\u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab\u001b[49m))\n\u001b[1;32m     20\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, train_loader, criterion, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=256, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "# Create model and optimizer\n",
    "model = SentimentRNN(len(vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, batch in tqdm(enumerate(train_loader), desc='Training'):\n",
    "            # Get data\n",
    "            inputs = batch['input_ids']\n",
    "            labels = batch['labels'].float()\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 1 == 0:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Train\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
