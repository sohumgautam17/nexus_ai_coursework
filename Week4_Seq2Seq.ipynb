{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "    \n",
    "def forward(self, x):\n",
    "    # x shape: (batch_size, seq_length)\n",
    "    embedded = self.embedding(x)\n",
    "    # embedded shape: (batch_size, seq_length, hidden_size)\n",
    "    \n",
    "    outputs, hidden = self.rnn(embedded)\n",
    "    return outputs, hidden\n",
    "\n",
    "# Define the Decoder\n",
    "class Decoder(nn.Module):\n",
    "def __init__(self, hidden_size, output_size, num_layers=1):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    \n",
    "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "    self.rnn = nn.GRU(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "def forward(self, x, hidden):\n",
    "    # x shape: (batch_size, 1)\n",
    "    x = x.unsqueeze(1)\n",
    "    embedded = self.embedding(x)\n",
    "    # embedded shape: (batch_size, 1, hidden_size)\n",
    "    \n",
    "    output, hidden = self.rnn(embedded, hidden)\n",
    "    prediction = self.out(output.squeeze(1))\n",
    "    return prediction, hidden\n",
    "\n",
    "# Define the Seq2Seq model\n",
    "class Seq2Seq(nn.Module):\n",
    "def __init__(self, encoder, decoder, device):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.device = device\n",
    "    \n",
    "def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "    batch_size = source.shape[0]\n",
    "    target_len = target.shape[1]\n",
    "    target_vocab_size = self.decoder.out.out_features\n",
    "    \n",
    "    outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_outputs, hidden = self.encoder(source)\n",
    "    \n",
    "    # First decoder input is the SOS token\n",
    "    decoder_input = torch.tensor([[SOS_token]] * batch_size).to(self.device)\n",
    "    \n",
    "    for t in range(1, target_len):\n",
    "        output, hidden = self.decoder(decoder_input, hidden)\n",
    "        outputs[:, t] = output\n",
    "        teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "        top1 = output.max(1)[1]\n",
    "        decoder_input = target[:, t] if teacher_force else top1\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "# Custom Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "def __init__(self, english_sentences, french_sentences, eng_vocab, fr_vocab):\n",
    "    self.english_sentences = english_sentences\n",
    "    self.french_sentences = french_sentences\n",
    "    self.eng_vocab = eng_vocab\n",
    "    self.fr_vocab = fr_vocab\n",
    "    \n",
    "def __len__(self):\n",
    "    return len(self.english_sentences)\n",
    "\n",
    "def __getitem__(self, idx):\n",
    "    eng_sent = self.english_sentences[idx]\n",
    "    fr_sent = self.french_sentences[idx]\n",
    "    \n",
    "    # Convert sentences to indices\n",
    "    eng_indices = [self.eng_vocab[word] for word in eng_sent.split()]\n",
    "    fr_indices = [self.fr_vocab[word] for word in fr_sent.split()]\n",
    "    \n",
    "    return torch.tensor(eng_indices), torch.tensor(fr_indices)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "model.train()\n",
    "total_loss = 0\n",
    "\n",
    "for batch_idx, (eng, fr) in enumerate(train_loader):\n",
    "    eng, fr = eng.to(device), fr.to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(eng, fr)\n",
    "    \n",
    "    output = output.view(-1, output.shape[-1])\n",
    "    fr = fr.view(-1)\n",
    "    \n",
    "    loss = criterion(output, fr)\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "return total_loss / len(train_loader)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create vocabulary (you'll need to implement this based on your data)\n",
    "# eng_vocab = create_vocabulary(english_sentences)\n",
    "# fr_vocab = create_vocabulary(french_sentences)\n",
    "\n",
    "# Create model\n",
    "encoder = Encoder(len(eng_vocab), HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "decoder = Decoder(HIDDEN_SIZE, len(fr_vocab), NUM_LAYERS).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TranslationDataset(english_sentences, french_sentences, eng_vocab, fr_vocab)\n",
    "train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
